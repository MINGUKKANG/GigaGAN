<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Scaling up GANs for Text-to-Image Synthesis">
  <meta name="keywords" content="GigaGAN, Text-to-Image, GAN, Image synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GigaGAN: Scaling up GANs for Text-to-Image Synthesis</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
</head>


<body>
  <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GigaGAN</h1>
          <h1 class="title is-2 publication-title">Scaling up GANs for Text-to-Image Synthesis</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://cvlab.postech.ac.kr/lab/">Minguk Kang</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://richzhang.github.io/">Richard Zhang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://jaesik.info/">Jaesik Park</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/sylvain-paris/">Sylvain Paris</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://taesung.me/">Taesung Park</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>POSTECH,</span>
            <span class="author-block"><sup>2</sup>Carnegie Mellon University,</span>
            <span class="author-block"><sup>3</sup>Adobe Research</span>
          </div>

          <div class="is-size-5 publication-venue">
            in CVPR 2023
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Giga-scale GAN for Text-to-Image Synthesis</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-t2i0 img-magnifier-container">
          <img id="myt2i0" src="./static/images/Text2img.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-t2i1 img-magnifier-container">
          <img id="myt2i1" src="./static/images/Text2img2.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-t2i2 img-magnifier-container">
          <img id="myt2i2" src="./static/images/Text2img.png"
          class="interpolation-image"/>
        </div>
      </div>
      <h2 class="subtitle has-text-justified">
        We achieve stable and scalable training of a 1B parameter GAN (GigaGAN) on LAION-2B dataset.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Upscaling to 16-megapixel photos with GigaGAN</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-iguana img-magnifier-container">
          <img id="myiguana" src="./static/images/iguana.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-dog img-magnifier-container">
          <img id="mydog" src="./static/images/upscale_dog.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-elephant img-magnifier-container">
          <img id="myelephant" src="./static/images/upscale_elephant.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-cat img-magnifier-container">
          <img id="mycat" src="./static/images/upscale_cat.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-pancake img-magnifier-container">
          <img id="mypancake" src="./static/images/upscale_pancake.png"
          class="interpolation-image"/>
        </div>
      </div>
      <h2 class="subtitle has-text-justified">
        Our GigaGAN upsampler can be used as an efficient, higher-quality upsampler for a diffusion-based text-to-image generator as well as a real-world image. 
        GigaGAN can synthesize ultra high-res images at 4k resolution in 3.66 seconds.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination.
            From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models.
            GANs used to be the de facto choice, with techniques like StyleGAN. With DALL&#183;E 2, auto-regressive and diffusion models became the new standard for large-scale generative models overnight.
            This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? 
            We find that na&#207;vely increasing the capacity of the StyleGAN architecture quickly becomes unstable.
            We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis.
            GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image.
            Second, it can synthesize high-resolution images, for example, 16-megapixel pixels in 3.66 seconds.
            Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/haIZPMBP-40"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Intra-prompt interpolation</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/inter_prompt_video.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Inter-prompt interpolation</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/inter_prompt_video.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">GigaGAN architecture</h2>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">1. GigaGAN generator</h3>
        <div class="content has-text-justified">
          <p>
            We extract text embeddings using a pretrained CLIP model and a learned encoder <b>T</b>.
            The local text descriptors are fed to the generator using cross-attention.
            The global text descriptor, along with a latent code <b>z</b>, is fed to a style mapping network <b>M</b> to produce style code <b>w</b>.
            The style code modulates the main generator using our style-adaptive kernel selection, shown on the right.
            The generator outputs an image pyramid by converting the intermediate features into RGB images. 
          </p>
        </div>
        <div class="content has-text-centered">
            <img src="./static/images/G_architecture.png"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
        </div>
        <!-- Prompt Interpolation image -->

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">2. GigaGAN discriminator</h3>
        <div class="content has-text-justified">
          <p>
            Our discriminator consists of two branches for processing the image and the text conditioning.
            The text branch processes the text similar to the generator. 
            The image branch receives an image pyramid and makes independent predictions for each image scale.
            Moreover, the predictions are made at all subsequent scales of the downsampling layers.
          </p>
        </div>
        <div class="content has-text-centered">
            <img src="./static/images/D_architecture1.png"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
        </div>
        <!-- Prompt Interpolation image -->

      </div>
    </div>

    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Latent space editing applications</h2>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">1. Prompt interpolation</h3>
        <div class="content has-text-justified">
          <p>
            GigaGAN enables smooth interpolation between prompts, as shown in the interpolation grid.
            The four corners are generated from the same latent z but with different text prompts.
          </p>
        </div>
          <!-- Prompt Interpolation image -->
          <div class="container is-max-desktop">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item item-sunflowers">
                <img src="./static/images/sunflowers.png"
                class="interpolation-image"/>
              </div>
              <div class="item item-victoria">
                <img src="./static/images/victorian_mansion.png"
                class="interpolation-image"/>
              </div>
              <div class="item item-house">
                <img src="./static/images/interpolation_house.png"
                class="interpolation-image"/>
              </div>
            </div>
          </div>
        <!-- Prompt Interpolation image -->
        
        <br>
        <h3 class="title is-4">2. Prompt mixing</h3>
        <div class="content has-text-justified">
          <p>
            GigaGAN retains a disentangled latent space, enabling us to combine the coarse style of one sample with the fine style of another.
            Moreover, GigaGAN can directly control the style with text prompts.
          </p>
        </div>
        <div class="content has-text-centered">
            <img src="./static/images/prompt_mixing.png"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
        </div>
        <!-- Prompt mixing -->

        <!-- Style swapping -->
        <h3 class="title is-4">3. Coarse-to-fine sytle swapping</h3>
        <div class="content has-text-justified">
          <p>
            Our GAN-based architecture retains a disentangled latent space, enabling us to blend the coarse style of one sample with the fine style of another.
          </p>
        </div>
        <!-- Prompt Interpolation image -->
        <div class="container is-max-desktop">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-male">
              <img src="./static/images/style_swapping_male.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-car">
              <img src="./static/images/style_swapping.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-house">
              <img src="./static/images/style_swapping_house.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>
      <!-- Prompt Interpolation image -->
      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Acknowledgements</h2>

        <div class="content has-text-justified">
          <p>
            We thank Simon Niklaus, Alexandru Chiculita, and Markus Woodson for building the distributed training pipeline. We thank Nupur Kumari, Gaurav Parmar, Bill Peebles, Phillip Isola, Alyosha Efros, and Joonghyuk Shin for their helpful comments.
            We also want to thank Chenlin Meng, Chitwan Saharia, and Jiahui Yu for answering many questions about their fantastic work. We thank Kevin Duarte for discussions regarding upsampling beyond 4K.
            Part of this work was done while Minguk Kang was an intern at Adobe Research.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kang2023gigagan,
  author    = {Kang, Minguk and Zhu, Jun-Yan and Zhang, Richard and Park, Jaesik and Shechtman, Eli and Paris, Sylvain and Park, Taesung},
  title     = {Scaling up GANs for Text-to-Image Synthesis},
  journal   = {arXiv preprint arXiv:xxxx.xxxxx},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  /* Initiate Magnify Function
  with the id of the image, and the strength of the magnifier glass:*/
  magnify("myiguana", 0);
  magnify("myiguana", 3);
  magnify("mydog", 3);
  magnify("myelephant", 3);
  magnify("mycat", 3);
  magnify("mypancake", 3);
</script>

</body>
</html>